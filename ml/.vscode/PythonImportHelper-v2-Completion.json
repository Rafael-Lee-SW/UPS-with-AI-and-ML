[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "LSTM",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "LSTM",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "MinMaxScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "MinMaxScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "MinMaxScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "MinMaxScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "LabelEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "MinMaxScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "ARIMA",
        "importPath": "statsmodels.tsa.arima.model",
        "description": "statsmodels.tsa.arima.model",
        "isExtraImport": true,
        "detail": "statsmodels.tsa.arima.model",
        "documentation": {}
    },
    {
        "label": "FPDF",
        "importPath": "fpdf",
        "description": "fpdf",
        "isExtraImport": true,
        "detail": "fpdf",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "joblib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "joblib",
        "description": "joblib",
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "matplotlib.font_manager",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.font_manager",
        "description": "matplotlib.font_manager",
        "detail": "matplotlib.font_manager",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "file_path = r'C:\\Users\\SSAFY\\Desktop\\data_ML\\purchase_transactions_2022_2023.csv'\nprint(\"Loading data...\")\ndata = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# Print to confirm the data is loaded correctly\nprint(\"Data loaded. Columns are:\")\nprint(data.columns)\n# Remove missing values\ndata.dropna(inplace=True)\n# Assign random locations (1-100) to each product\nnp.random.seed(42)",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "data = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# Print to confirm the data is loaded correctly\nprint(\"Data loaded. Columns are:\")\nprint(data.columns)\n# Remove missing values\ndata.dropna(inplace=True)\n# Assign random locations (1-100) to each product\nnp.random.seed(42)\nproduct_ids = data['product_code'].unique()\nlocation_mapping = {pid: np.random.randint(1, 101) for pid in product_ids}",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "product_ids",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "product_ids = data['product_code'].unique()\nlocation_mapping = {pid: np.random.randint(1, 101) for pid in product_ids}\ndata['location'] = data['product_code'].map(location_mapping)\n# Ensure the 'purchase_date' column is included and set as index\ndata = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=False)  # Keep 'purchase_date' column, don't set as index\n# Extract necessary columns (including purchase_date)\ndata = data[['purchase_date', 'product_code', 'item_description', 'master_category_full_name', 'price', 'sales_unit', 'location']]\n# Check the first few rows to ensure the data is correct\nprint(\"Processed data preview:\")",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "location_mapping",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "location_mapping = {pid: np.random.randint(1, 101) for pid in product_ids}\ndata['location'] = data['product_code'].map(location_mapping)\n# Ensure the 'purchase_date' column is included and set as index\ndata = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=False)  # Keep 'purchase_date' column, don't set as index\n# Extract necessary columns (including purchase_date)\ndata = data[['purchase_date', 'product_code', 'item_description', 'master_category_full_name', 'price', 'sales_unit', 'location']]\n# Check the first few rows to ensure the data is correct\nprint(\"Processed data preview:\")\nprint(data.head())",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "data['location']",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "data['location'] = data['product_code'].map(location_mapping)\n# Ensure the 'purchase_date' column is included and set as index\ndata = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=False)  # Keep 'purchase_date' column, don't set as index\n# Extract necessary columns (including purchase_date)\ndata = data[['purchase_date', 'product_code', 'item_description', 'master_category_full_name', 'price', 'sales_unit', 'location']]\n# Check the first few rows to ensure the data is correct\nprint(\"Processed data preview:\")\nprint(data.head())\n# Save the prepared data (with 'purchase_date' column)",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "data = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=False)  # Keep 'purchase_date' column, don't set as index\n# Extract necessary columns (including purchase_date)\ndata = data[['purchase_date', 'product_code', 'item_description', 'master_category_full_name', 'price', 'sales_unit', 'location']]\n# Check the first few rows to ensure the data is correct\nprint(\"Processed data preview:\")\nprint(data.head())\n# Save the prepared data (with 'purchase_date' column)\noutput_file = 'prepared_sales_data.csv'\ndata.to_csv(output_file, index=False)",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "data = data[['purchase_date', 'product_code', 'item_description', 'master_category_full_name', 'price', 'sales_unit', 'location']]\n# Check the first few rows to ensure the data is correct\nprint(\"Processed data preview:\")\nprint(data.head())\n# Save the prepared data (with 'purchase_date' column)\noutput_file = 'prepared_sales_data.csv'\ndata.to_csv(output_file, index=False)\n# Confirm that the file was saved successfully\nprint(f\"Data has been saved to {output_file}\")",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "output_file",
        "kind": 5,
        "importPath": "data_preparation",
        "description": "data_preparation",
        "peekOfCode": "output_file = 'prepared_sales_data.csv'\ndata.to_csv(output_file, index=False)\n# Confirm that the file was saved successfully\nprint(f\"Data has been saved to {output_file}\")",
        "detail": "data_preparation",
        "documentation": {}
    },
    {
        "label": "create_sequences",
        "kind": 2,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "def create_sequences(data, seq_length):\n    xs = []\n    ys = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        y = data[i + seq_length]\n        xs.append(x)\n        ys.append(y)\n    return np.array(xs), np.array(ys)\nX, y = create_sequences(scaled_data, SEQ_LENGTH)",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "file_path = r'C:\\Users\\SSAFY\\Desktop\\data_ML\\purchase_transactions_2022_2023.csv'\ndata = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# 결측치 제거\ndata.dropna(inplace=True)\n# 날짜로 정렬 및 인덱스로 설정\ndata = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=True)\n# 열 이름 확인\nprint(data.columns)\n# 'price' 또는 'sales_unit' 열을 사용하여 예측",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "data = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# 결측치 제거\ndata.dropna(inplace=True)\n# 날짜로 정렬 및 인덱스로 설정\ndata = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=True)\n# 열 이름 확인\nprint(data.columns)\n# 'price' 또는 'sales_unit' 열을 사용하여 예측\n# 여기서는 'price'를 예시로 사용",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "data = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=True)\n# 열 이름 확인\nprint(data.columns)\n# 'price' 또는 'sales_unit' 열을 사용하여 예측\n# 여기서는 'price'를 예시로 사용\nsales_data = data[['price']].values  # 또는 'sales_unit'를 사용할 수 있습니다.\n# 데이터 스케일링\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(sales_data)",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "sales_data",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "sales_data = data[['price']].values  # 또는 'sales_unit'를 사용할 수 있습니다.\n# 데이터 스케일링\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(sales_data)\n# 시퀀스 생성\nSEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    ys = []\n    for i in range(len(data) - seq_length):",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "scaler",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "scaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(sales_data)\n# 시퀀스 생성\nSEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    ys = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        y = data[i + seq_length]",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "scaled_data",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "scaled_data = scaler.fit_transform(sales_data)\n# 시퀀스 생성\nSEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    ys = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        y = data[i + seq_length]\n        xs.append(x)",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "SEQ_LENGTH",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "SEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    ys = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        y = data[i + seq_length]\n        xs.append(x)\n        ys.append(y)\n    return np.array(xs), np.array(ys)",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "split = int(0.8 * len(X))\nX_train, X_test = X[:split], X[split:]\ny_train, y_test = y[:split], y[split:]\n# 모델 구축\nmodel = Sequential()\nmodel.add(LSTM(units=50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel.add(Dense(units=1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n# 모델 학습\nhistory = model.fit(",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "model = Sequential()\nmodel.add(LSTM(units=50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel.add(Dense(units=1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n# 모델 학습\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_test, y_test)",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "history = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_test, y_test)\n)\n# 모델 평가\ny_pred = model.predict(X_test)\ny_test_inv = scaler.inverse_transform(y_test)\ny_pred_inv = scaler.inverse_transform(y_pred)",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "y_pred",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "y_pred = model.predict(X_test)\ny_test_inv = scaler.inverse_transform(y_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\nplt.figure(figsize=(10,6))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.title('Actual vs Predicted Sales')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.legend()",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "y_test_inv",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "y_test_inv = scaler.inverse_transform(y_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\nplt.figure(figsize=(10,6))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.title('Actual vs Predicted Sales')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.legend()\nplt.show()",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "y_pred_inv",
        "kind": 5,
        "importPath": "kerasML",
        "description": "kerasML",
        "peekOfCode": "y_pred_inv = scaler.inverse_transform(y_pred)\nplt.figure(figsize=(10,6))\nplt.plot(y_test_inv, label='Actual')\nplt.plot(y_pred_inv, label='Predicted')\nplt.title('Actual vs Predicted Sales')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.legend()\nplt.show()\n# 모델 저장",
        "detail": "kerasML",
        "documentation": {}
    },
    {
        "label": "create_sequences",
        "kind": 2,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "def create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        xs.append(x)\n    return np.array(xs)\nX_test_new = create_sequences(scaled_data, SEQ_LENGTH)\n# Make predictions using the loaded model\ny_pred_new = model.predict(X_test_new)\n# Inverse transform the predictions to get actual prices",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "model = tf.keras.models.load_model('sales_forecast_model.h5')\n# Example: Test the model with some new data (make sure to preprocess it similarly to your training data)\n# For example, let's assume you're testing with a similar structure of 30 days of data.\n# You can generate synthetic test data, or load real data from your dataset.\n# Load new data (same structure as training data)\nfile_path = r'C:\\Users\\SSAFY\\Desktop\\data_ML\\purchase_transactions_2022_2023.csv'\ndata = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# Data preprocessing\ndata.dropna(inplace=True)\ndata = data.sort_values('purchase_date')",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "file_path = r'C:\\Users\\SSAFY\\Desktop\\data_ML\\purchase_transactions_2022_2023.csv'\ndata = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# Data preprocessing\ndata.dropna(inplace=True)\ndata = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=True)\n# Assume 'price' is the target column\nscaler = MinMaxScaler()\nsales_data = data[['price']].values\nscaled_data = scaler.fit_transform(sales_data)",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "data = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# Data preprocessing\ndata.dropna(inplace=True)\ndata = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=True)\n# Assume 'price' is the target column\nscaler = MinMaxScaler()\nsales_data = data[['price']].values\nscaled_data = scaler.fit_transform(sales_data)\n# Let's create sequences from the test data (like you did during training)",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "data = data.sort_values('purchase_date')\ndata.set_index('purchase_date', inplace=True)\n# Assume 'price' is the target column\nscaler = MinMaxScaler()\nsales_data = data[['price']].values\nscaled_data = scaler.fit_transform(sales_data)\n# Let's create sequences from the test data (like you did during training)\nSEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "scaler",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "scaler = MinMaxScaler()\nsales_data = data[['price']].values\nscaled_data = scaler.fit_transform(sales_data)\n# Let's create sequences from the test data (like you did during training)\nSEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        xs.append(x)",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "sales_data",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "sales_data = data[['price']].values\nscaled_data = scaler.fit_transform(sales_data)\n# Let's create sequences from the test data (like you did during training)\nSEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        xs.append(x)\n    return np.array(xs)",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "scaled_data",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "scaled_data = scaler.fit_transform(sales_data)\n# Let's create sequences from the test data (like you did during training)\nSEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        xs.append(x)\n    return np.array(xs)\nX_test_new = create_sequences(scaled_data, SEQ_LENGTH)",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "SEQ_LENGTH",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "SEQ_LENGTH = 30\ndef create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i + seq_length)]\n        xs.append(x)\n    return np.array(xs)\nX_test_new = create_sequences(scaled_data, SEQ_LENGTH)\n# Make predictions using the loaded model\ny_pred_new = model.predict(X_test_new)",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "X_test_new",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "X_test_new = create_sequences(scaled_data, SEQ_LENGTH)\n# Make predictions using the loaded model\ny_pred_new = model.predict(X_test_new)\n# Inverse transform the predictions to get actual prices\ny_pred_new_inv = scaler.inverse_transform(y_pred_new)\n# Print or plot the predictions\nprint(\"Predicted Sales:\")\nprint(y_pred_new_inv[:10])  # Print the first 10 predictions\n# Visualize\nimport matplotlib.pyplot as plt",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "y_pred_new",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "y_pred_new = model.predict(X_test_new)\n# Inverse transform the predictions to get actual prices\ny_pred_new_inv = scaler.inverse_transform(y_pred_new)\n# Print or plot the predictions\nprint(\"Predicted Sales:\")\nprint(y_pred_new_inv[:10])  # Print the first 10 predictions\n# Visualize\nimport matplotlib.pyplot as plt\nplt.plot(y_pred_new_inv[:100])  # Plot the first 100 predictions\nplt.title('Predicted Sales')",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "y_pred_new_inv",
        "kind": 5,
        "importPath": "loadModel",
        "description": "loadModel",
        "peekOfCode": "y_pred_new_inv = scaler.inverse_transform(y_pred_new)\n# Print or plot the predictions\nprint(\"Predicted Sales:\")\nprint(y_pred_new_inv[:10])  # Print the first 10 predictions\n# Visualize\nimport matplotlib.pyplot as plt\nplt.plot(y_pred_new_inv[:100])  # Plot the first 100 predictions\nplt.title('Predicted Sales')\nplt.xlabel('Time Step')\nplt.ylabel('Sales')",
        "detail": "loadModel",
        "documentation": {}
    },
    {
        "label": "prepare_product_data",
        "kind": 2,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "def prepare_product_data(product_code, data, target_column, seq_length):\n    product_data = data[data['product_code'] == product_code].copy()\n    product_data.sort_values('purchase_date', inplace=True)\n    sales_values = product_data[[target_column]].values\n    # Scale data\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(sales_values)\n    # Create sequences\n    X, y = [], []\n    for i in range(len(scaled_data) - seq_length):",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "data = pd.read_csv('prepared_sales_data.csv', parse_dates=['purchase_date'])\n# Function to prepare data for a specific product\ndef prepare_product_data(product_code, data, target_column, seq_length):\n    product_data = data[data['product_code'] == product_code].copy()\n    product_data.sort_values('purchase_date', inplace=True)\n    sales_values = product_data[[target_column]].values\n    # Scale data\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(sales_values)\n    # Create sequences",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "SEQ_LENGTH",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "SEQ_LENGTH = 30\nTARGET_COLUMN = 'price'  # Change to 'sales_unit' for units prediction\n# Get unique product codes\nproduct_codes = data['product_code'].unique()\n# We'll demonstrate with the top-selling product\ntop_product_code = data.groupby('product_code')['price'].sum().idxmax()\n# Prepare data for the top product\nX_train, X_test, y_train, y_test, scaler = prepare_product_data(top_product_code, data, TARGET_COLUMN, SEQ_LENGTH)\n# Build LSTM model\nmodel = Sequential()",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "TARGET_COLUMN",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "TARGET_COLUMN = 'price'  # Change to 'sales_unit' for units prediction\n# Get unique product codes\nproduct_codes = data['product_code'].unique()\n# We'll demonstrate with the top-selling product\ntop_product_code = data.groupby('product_code')['price'].sum().idxmax()\n# Prepare data for the top product\nX_train, X_test, y_train, y_test, scaler = prepare_product_data(top_product_code, data, TARGET_COLUMN, SEQ_LENGTH)\n# Build LSTM model\nmodel = Sequential()\nmodel.add(LSTM(units=50, activation='relu', input_shape=(SEQ_LENGTH, 1)))",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "product_codes",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "product_codes = data['product_code'].unique()\n# We'll demonstrate with the top-selling product\ntop_product_code = data.groupby('product_code')['price'].sum().idxmax()\n# Prepare data for the top product\nX_train, X_test, y_train, y_test, scaler = prepare_product_data(top_product_code, data, TARGET_COLUMN, SEQ_LENGTH)\n# Build LSTM model\nmodel = Sequential()\nmodel.add(LSTM(units=50, activation='relu', input_shape=(SEQ_LENGTH, 1)))\nmodel.add(Dense(units=1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "top_product_code",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "top_product_code = data.groupby('product_code')['price'].sum().idxmax()\n# Prepare data for the top product\nX_train, X_test, y_train, y_test, scaler = prepare_product_data(top_product_code, data, TARGET_COLUMN, SEQ_LENGTH)\n# Build LSTM model\nmodel = Sequential()\nmodel.add(LSTM(units=50, activation='relu', input_shape=(SEQ_LENGTH, 1)))\nmodel.add(Dense(units=1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n# Train the model\nhistory = model.fit(",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "model = Sequential()\nmodel.add(LSTM(units=50, activation='relu', input_shape=(SEQ_LENGTH, 1)))\nmodel.add(Dense(units=1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=20,\n    batch_size=32,\n    validation_data=(X_test, y_test)",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "history = model.fit(\n    X_train, y_train,\n    epochs=20,\n    batch_size=32,\n    validation_data=(X_test, y_test)\n)\n# Evaluate the model\ny_pred = model.predict(X_test)\ny_test_inv = scaler.inverse_transform(y_test)\ny_pred_inv = scaler.inverse_transform(y_pred)",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "y_pred",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "y_pred = model.predict(X_test)\ny_test_inv = scaler.inverse_transform(y_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nprint(f\"LSTM Model MSE: {mse}\")\n# Save the model\nmodel.save('lstm_sales_forecast_model.h5')",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "y_test_inv",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "y_test_inv = scaler.inverse_transform(y_test)\ny_pred_inv = scaler.inverse_transform(y_pred)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nprint(f\"LSTM Model MSE: {mse}\")\n# Save the model\nmodel.save('lstm_sales_forecast_model.h5')",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "y_pred_inv",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "y_pred_inv = scaler.inverse_transform(y_pred)\nmse = mean_squared_error(y_test_inv, y_pred_inv)\nprint(f\"LSTM Model MSE: {mse}\")\n# Save the model\nmodel.save('lstm_sales_forecast_model.h5')",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "mse",
        "kind": 5,
        "importPath": "lstm_model",
        "description": "lstm_model",
        "peekOfCode": "mse = mean_squared_error(y_test_inv, y_pred_inv)\nprint(f\"LSTM Model MSE: {mse}\")\n# Save the model\nmodel.save('lstm_sales_forecast_model.h5')",
        "detail": "lstm_model",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "file_path = r'C:\\Users\\SSAFY\\Desktop\\data_ML\\purchase_transactions_2022_2023.csv'\ndata = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# Fill missing values if necessary\ndata['price'] = data['price'].fillna(data['price'].mean())\ndata['sales_unit'] = data['sales_unit'].fillna(data['sales_unit'].mean())\n# Calculate total sales amount per transaction\ndata['total_sales'] = data['price'] * data['sales_unit']\n# Chapter 1: Analyze the Top 100 Selling Products\n# Aggregate total sales per product\nproduct_sales = data.groupby('product_code')['total_sales'].sum().reset_index()",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "data = pd.read_csv(file_path, parse_dates=['purchase_date'])\n# Fill missing values if necessary\ndata['price'] = data['price'].fillna(data['price'].mean())\ndata['sales_unit'] = data['sales_unit'].fillna(data['sales_unit'].mean())\n# Calculate total sales amount per transaction\ndata['total_sales'] = data['price'] * data['sales_unit']\n# Chapter 1: Analyze the Top 100 Selling Products\n# Aggregate total sales per product\nproduct_sales = data.groupby('product_code')['total_sales'].sum().reset_index()\n# Select the top 100 selling products",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "data['price']",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "data['price'] = data['price'].fillna(data['price'].mean())\ndata['sales_unit'] = data['sales_unit'].fillna(data['sales_unit'].mean())\n# Calculate total sales amount per transaction\ndata['total_sales'] = data['price'] * data['sales_unit']\n# Chapter 1: Analyze the Top 100 Selling Products\n# Aggregate total sales per product\nproduct_sales = data.groupby('product_code')['total_sales'].sum().reset_index()\n# Select the top 100 selling products\ntop_100_products = product_sales.sort_values(by='total_sales', ascending=False).head(100)\ntop_100_product_codes = top_100_products['product_code'].tolist()",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "data['sales_unit']",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "data['sales_unit'] = data['sales_unit'].fillna(data['sales_unit'].mean())\n# Calculate total sales amount per transaction\ndata['total_sales'] = data['price'] * data['sales_unit']\n# Chapter 1: Analyze the Top 100 Selling Products\n# Aggregate total sales per product\nproduct_sales = data.groupby('product_code')['total_sales'].sum().reset_index()\n# Select the top 100 selling products\ntop_100_products = product_sales.sort_values(by='total_sales', ascending=False).head(100)\ntop_100_product_codes = top_100_products['product_code'].tolist()\n# Initialize PDF Report",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "data['total_sales']",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "data['total_sales'] = data['price'] * data['sales_unit']\n# Chapter 1: Analyze the Top 100 Selling Products\n# Aggregate total sales per product\nproduct_sales = data.groupby('product_code')['total_sales'].sum().reset_index()\n# Select the top 100 selling products\ntop_100_products = product_sales.sort_values(by='total_sales', ascending=False).head(100)\ntop_100_product_codes = top_100_products['product_code'].tolist()\n# Initialize PDF Report\npdf = FPDF()\npdf.set_auto_page_break(auto=True, margin=15)",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "product_sales",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "product_sales = data.groupby('product_code')['total_sales'].sum().reset_index()\n# Select the top 100 selling products\ntop_100_products = product_sales.sort_values(by='total_sales', ascending=False).head(100)\ntop_100_product_codes = top_100_products['product_code'].tolist()\n# Initialize PDF Report\npdf = FPDF()\npdf.set_auto_page_break(auto=True, margin=15)\n# Add first page for summary\npdf.add_page()\npdf.set_font(\"Arial\", 'B', 16)",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "top_100_products",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "top_100_products = product_sales.sort_values(by='total_sales', ascending=False).head(100)\ntop_100_product_codes = top_100_products['product_code'].tolist()\n# Initialize PDF Report\npdf = FPDF()\npdf.set_auto_page_break(auto=True, margin=15)\n# Add first page for summary\npdf.add_page()\npdf.set_font(\"Arial\", 'B', 16)\npdf.cell(200, 10, txt=\"Top 100 Products Sales Analysis Report\", ln=True, align='C')\n# Forecast sales for the top 100 products and generate insights",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "top_100_product_codes",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "top_100_product_codes = top_100_products['product_code'].tolist()\n# Initialize PDF Report\npdf = FPDF()\npdf.set_auto_page_break(auto=True, margin=15)\n# Add first page for summary\npdf.add_page()\npdf.set_font(\"Arial\", 'B', 16)\npdf.cell(200, 10, txt=\"Top 100 Products Sales Analysis Report\", ln=True, align='C')\n# Forecast sales for the top 100 products and generate insights\nforecast_horizon = 30  # Forecast for 30 days",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "pdf",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "pdf = FPDF()\npdf.set_auto_page_break(auto=True, margin=15)\n# Add first page for summary\npdf.add_page()\npdf.set_font(\"Arial\", 'B', 16)\npdf.cell(200, 10, txt=\"Top 100 Products Sales Analysis Report\", ln=True, align='C')\n# Forecast sales for the top 100 products and generate insights\nforecast_horizon = 30  # Forecast for 30 days\nvalid_forecasts = 0\nfor product in top_100_product_codes:",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "forecast_horizon",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "forecast_horizon = 30  # Forecast for 30 days\nvalid_forecasts = 0\nfor product in top_100_product_codes:\n    product_data = data[data['product_code'] == product]\n    product_time_series = product_data.set_index('purchase_date').resample('D')['total_sales'].sum()\n    # Only proceed if enough data points are available\n    if len(product_time_series.dropna()) >= 30:\n        # Fit ARIMA model and forecast\n        model = ARIMA(product_time_series, order=(1, 1, 1))\n        model_fit = model.fit()",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "valid_forecasts",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "valid_forecasts = 0\nfor product in top_100_product_codes:\n    product_data = data[data['product_code'] == product]\n    product_time_series = product_data.set_index('purchase_date').resample('D')['total_sales'].sum()\n    # Only proceed if enough data points are available\n    if len(product_time_series.dropna()) >= 30:\n        # Fit ARIMA model and forecast\n        model = ARIMA(product_time_series, order=(1, 1, 1))\n        model_fit = model.fit()\n        # Forecast future sales",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "age_group_sales",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "age_group_sales = data.groupby('age_group')['total_sales'].sum().reset_index()\n# Plot Sales by Age Group\nplt.figure(figsize=(8, 6))\nplt.bar(age_group_sales['age_group'].astype(str), age_group_sales['total_sales'])\nplt.xlabel('Age Group')\nplt.ylabel('Total Sales')\nplt.title('Sales by Age Group')\nplt.grid(True)\nplt.tight_layout()\nplt.savefig('sales_by_age_group.png')",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "gender_sales",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "gender_sales = data.groupby('person_gender')['total_sales'].sum().reset_index()\ngender_mapping = {0: 'Female', 1: 'Male', -1: 'Unknown'}\ngender_sales['person_gender'] = gender_sales['person_gender'].map(gender_mapping)\n# Plot Sales by Gender\nplt.figure(figsize=(6, 6))\nplt.pie(gender_sales['total_sales'], labels=gender_sales['person_gender'], autopct='%1.1f%%', startangle=90)\nplt.title('Sales by Gender')\nplt.tight_layout()\nplt.savefig('sales_by_gender.png')\nplt.close()",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "gender_mapping",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "gender_mapping = {0: 'Female', 1: 'Male', -1: 'Unknown'}\ngender_sales['person_gender'] = gender_sales['person_gender'].map(gender_mapping)\n# Plot Sales by Gender\nplt.figure(figsize=(6, 6))\nplt.pie(gender_sales['total_sales'], labels=gender_sales['person_gender'], autopct='%1.1f%%', startangle=90)\nplt.title('Sales by Gender')\nplt.tight_layout()\nplt.savefig('sales_by_gender.png')\nplt.close()\n# Add Gender Sales Graph to PDF",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "gender_sales['person_gender']",
        "kind": 5,
        "importPath": "ml",
        "description": "ml",
        "peekOfCode": "gender_sales['person_gender'] = gender_sales['person_gender'].map(gender_mapping)\n# Plot Sales by Gender\nplt.figure(figsize=(6, 6))\nplt.pie(gender_sales['total_sales'], labels=gender_sales['person_gender'], autopct='%1.1f%%', startangle=90)\nplt.title('Sales by Gender')\nplt.tight_layout()\nplt.savefig('sales_by_gender.png')\nplt.close()\n# Add Gender Sales Graph to PDF\npdf.add_page()",
        "detail": "ml",
        "documentation": {}
    },
    {
        "label": "search_and_forecast",
        "kind": 2,
        "importPath": "product_search",
        "description": "product_search",
        "peekOfCode": "def search_and_forecast(product_name):\n    product_data = data[data['item_description'].str.contains(product_name, case=False)]\n    if product_data.empty:\n        print(\"Product not found.\")\n        return\n    product_desc = product_data['item_description'].iloc[0]\n    product_data = product_data.sort_values('purchase_date')\n    sales_values = product_data[['price']].values\n    # Prepare data for forecasting\n    scaler = MinMaxScaler()",
        "detail": "product_search",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "product_search",
        "description": "product_search",
        "peekOfCode": "data = pd.read_csv('prepared_sales_data.csv', parse_dates=['purchase_date'])\nlstm_model = tf.keras.models.load_model('lstm_sales_forecast_model.h5')\ndef search_and_forecast(product_name):\n    product_data = data[data['item_description'].str.contains(product_name, case=False)]\n    if product_data.empty:\n        print(\"Product not found.\")\n        return\n    product_desc = product_data['item_description'].iloc[0]\n    product_data = product_data.sort_values('purchase_date')\n    sales_values = product_data[['price']].values",
        "detail": "product_search",
        "documentation": {}
    },
    {
        "label": "lstm_model",
        "kind": 5,
        "importPath": "product_search",
        "description": "product_search",
        "peekOfCode": "lstm_model = tf.keras.models.load_model('lstm_sales_forecast_model.h5')\ndef search_and_forecast(product_name):\n    product_data = data[data['item_description'].str.contains(product_name, case=False)]\n    if product_data.empty:\n        print(\"Product not found.\")\n        return\n    product_desc = product_data['item_description'].iloc[0]\n    product_data = product_data.sort_values('purchase_date')\n    sales_values = product_data[['price']].values\n    # Prepare data for forecasting",
        "detail": "product_search",
        "documentation": {}
    },
    {
        "label": "product_name",
        "kind": 5,
        "importPath": "product_search",
        "description": "product_search",
        "peekOfCode": "product_name = input(\"Enter product name to search: \")\nsearch_and_forecast(product_name)",
        "detail": "product_search",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "data = pd.read_csv('prepared_sales_data.csv', parse_dates=['purchase_date'])\n# Print column names to confirm data loaded correctly\nprint(\"Data loaded successfully. Columns are:\")\nprint(data.columns)\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ndata['product_code_encoded'] = label_encoder.fit_transform(data['product_code'])\ndata['category_encoded'] = label_encoder.fit_transform(data['master_category_full_name'])\n# Features and target\nfeatures = ['product_code_encoded', 'category_encoded', 'location']",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "label_encoder",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "label_encoder = LabelEncoder()\ndata['product_code_encoded'] = label_encoder.fit_transform(data['product_code'])\ndata['category_encoded'] = label_encoder.fit_transform(data['master_category_full_name'])\n# Features and target\nfeatures = ['product_code_encoded', 'category_encoded', 'location']\ntarget = 'price'  # You can change this to 'sales_unit' if needed\n# Print features and target details\nprint(f\"Features: {features}, Target: {target}\")\n# Prepare data\nX = data[features]",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "data['product_code_encoded']",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "data['product_code_encoded'] = label_encoder.fit_transform(data['product_code'])\ndata['category_encoded'] = label_encoder.fit_transform(data['master_category_full_name'])\n# Features and target\nfeatures = ['product_code_encoded', 'category_encoded', 'location']\ntarget = 'price'  # You can change this to 'sales_unit' if needed\n# Print features and target details\nprint(f\"Features: {features}, Target: {target}\")\n# Prepare data\nX = data[features]\ny = data[target]",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "data['category_encoded']",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "data['category_encoded'] = label_encoder.fit_transform(data['master_category_full_name'])\n# Features and target\nfeatures = ['product_code_encoded', 'category_encoded', 'location']\ntarget = 'price'  # You can change this to 'sales_unit' if needed\n# Print features and target details\nprint(f\"Features: {features}, Target: {target}\")\n# Prepare data\nX = data[features]\ny = data[target]\n# Split into train and test sets",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "features",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "features = ['product_code_encoded', 'category_encoded', 'location']\ntarget = 'price'  # You can change this to 'sales_unit' if needed\n# Print features and target details\nprint(f\"Features: {features}, Target: {target}\")\n# Prepare data\nX = data[features]\ny = data[target]\n# Split into train and test sets\nsplit = int(0.8 * len(X))\nX_train, X_test = X.iloc[:split], X.iloc[split:]",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "target",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "target = 'price'  # You can change this to 'sales_unit' if needed\n# Print features and target details\nprint(f\"Features: {features}, Target: {target}\")\n# Prepare data\nX = data[features]\ny = data[target]\n# Split into train and test sets\nsplit = int(0.8 * len(X))\nX_train, X_test = X.iloc[:split], X.iloc[split:]\ny_train, y_test = y.iloc[:split], y.iloc[split:]",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "X = data[features]\ny = data[target]\n# Split into train and test sets\nsplit = int(0.8 * len(X))\nX_train, X_test = X.iloc[:split], X.iloc[split:]\ny_train, y_test = y.iloc[:split], y.iloc[split:]\n# Print the shape of the training and test sets\nprint(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n# Build Random Forest model\nprint(\"Training the Random Forest model...\")",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "y = data[target]\n# Split into train and test sets\nsplit = int(0.8 * len(X))\nX_train, X_test = X.iloc[:split], X.iloc[split:]\ny_train, y_test = y.iloc[:split], y.iloc[split:]\n# Print the shape of the training and test sets\nprint(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n# Build Random Forest model\nprint(\"Training the Random Forest model...\")\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "split = int(0.8 * len(X))\nX_train, X_test = X.iloc[:split], X.iloc[split:]\ny_train, y_test = y.iloc[:split], y.iloc[split:]\n# Print the shape of the training and test sets\nprint(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n# Build Random Forest model\nprint(\"Training the Random Forest model...\")\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n# Save the trained Random Forest model",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "rf_model",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n# Save the trained Random Forest model\njoblib.dump(rf_model, 'random_forest_sales_model.pkl')\nprint(\"Model saved as 'random_forest_sales_model.pkl'\")\n# Evaluate the model\nprint(\"Evaluating the model...\")\ny_pred = rf_model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\n# Print the MSE to see the result",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "y_pred",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "y_pred = rf_model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\n# Print the MSE to see the result\nprint(f\"Random Forest Model MSE: {mse}\")\n# Save the predictions and actual values to a CSV file\npredictions_df = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred\n})\npredictions_df.to_csv('random_forest_predictions.csv', index=False)",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "mse",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "mse = mean_squared_error(y_test, y_pred)\n# Print the MSE to see the result\nprint(f\"Random Forest Model MSE: {mse}\")\n# Save the predictions and actual values to a CSV file\npredictions_df = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred\n})\npredictions_df.to_csv('random_forest_predictions.csv', index=False)\nprint(\"Predictions saved to 'random_forest_predictions.csv'\")",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "predictions_df",
        "kind": 5,
        "importPath": "random_forest_model",
        "description": "random_forest_model",
        "peekOfCode": "predictions_df = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred\n})\npredictions_df.to_csv('random_forest_predictions.csv', index=False)\nprint(\"Predictions saved to 'random_forest_predictions.csv'\")",
        "detail": "random_forest_model",
        "documentation": {}
    },
    {
        "label": "sanitize_filename",
        "kind": 2,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "def sanitize_filename(filename):\n    return re.sub(r'[<>:\"/\\\\|?* ]', '_', filename)\n# Load data and models\ndata = pd.read_csv('prepared_sales_data.csv', parse_dates=['purchase_date'])\nlstm_model = tf.keras.models.load_model('lstm_sales_forecast_model.h5')  # Load LSTM model\n# Generate Top 10 products by sales amount\ntop_products = data.groupby('item_description')['price'].sum().sort_values(ascending=False).head(10)\ntop_product_descriptions = top_products.index.tolist()\n# Generate Top 10 locations by sales amount\ntop_locations = data.groupby('location')['price'].sum().sort_values(ascending=False).head(10)",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "plt.rcParams['font.family']",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "plt.rcParams['font.family'] = 'Malgun Gothic'\nplt.rcParams['axes.unicode_minus'] = False  # To handle minus signs correctly\n# Function to sanitize file names by replacing invalid characters\ndef sanitize_filename(filename):\n    return re.sub(r'[<>:\"/\\\\|?* ]', '_', filename)\n# Load data and models\ndata = pd.read_csv('prepared_sales_data.csv', parse_dates=['purchase_date'])\nlstm_model = tf.keras.models.load_model('lstm_sales_forecast_model.h5')  # Load LSTM model\n# Generate Top 10 products by sales amount\ntop_products = data.groupby('item_description')['price'].sum().sort_values(ascending=False).head(10)",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "plt.rcParams['axes.unicode_minus']",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "plt.rcParams['axes.unicode_minus'] = False  # To handle minus signs correctly\n# Function to sanitize file names by replacing invalid characters\ndef sanitize_filename(filename):\n    return re.sub(r'[<>:\"/\\\\|?* ]', '_', filename)\n# Load data and models\ndata = pd.read_csv('prepared_sales_data.csv', parse_dates=['purchase_date'])\nlstm_model = tf.keras.models.load_model('lstm_sales_forecast_model.h5')  # Load LSTM model\n# Generate Top 10 products by sales amount\ntop_products = data.groupby('item_description')['price'].sum().sort_values(ascending=False).head(10)\ntop_product_descriptions = top_products.index.tolist()",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "data = pd.read_csv('prepared_sales_data.csv', parse_dates=['purchase_date'])\nlstm_model = tf.keras.models.load_model('lstm_sales_forecast_model.h5')  # Load LSTM model\n# Generate Top 10 products by sales amount\ntop_products = data.groupby('item_description')['price'].sum().sort_values(ascending=False).head(10)\ntop_product_descriptions = top_products.index.tolist()\n# Generate Top 10 locations by sales amount\ntop_locations = data.groupby('location')['price'].sum().sort_values(ascending=False).head(10)\n# Generate Top 10 categories by sales amount\ntop_categories = data.groupby('master_category_full_name')['price'].sum().sort_values(ascending=False).head(10)\ntop_category_names = top_categories.index.tolist()",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "lstm_model",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "lstm_model = tf.keras.models.load_model('lstm_sales_forecast_model.h5')  # Load LSTM model\n# Generate Top 10 products by sales amount\ntop_products = data.groupby('item_description')['price'].sum().sort_values(ascending=False).head(10)\ntop_product_descriptions = top_products.index.tolist()\n# Generate Top 10 locations by sales amount\ntop_locations = data.groupby('location')['price'].sum().sort_values(ascending=False).head(10)\n# Generate Top 10 categories by sales amount\ntop_categories = data.groupby('master_category_full_name')['price'].sum().sort_values(ascending=False).head(10)\ntop_category_names = top_categories.index.tolist()\n# Generate forecasts for each top product",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "top_products",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "top_products = data.groupby('item_description')['price'].sum().sort_values(ascending=False).head(10)\ntop_product_descriptions = top_products.index.tolist()\n# Generate Top 10 locations by sales amount\ntop_locations = data.groupby('location')['price'].sum().sort_values(ascending=False).head(10)\n# Generate Top 10 categories by sales amount\ntop_categories = data.groupby('master_category_full_name')['price'].sum().sort_values(ascending=False).head(10)\ntop_category_names = top_categories.index.tolist()\n# Generate forecasts for each top product\nreport_date = datetime.now().strftime('%Y-%m-%d')\nwith open('sales_report.md', 'w', encoding='utf-8') as f:",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "top_product_descriptions",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "top_product_descriptions = top_products.index.tolist()\n# Generate Top 10 locations by sales amount\ntop_locations = data.groupby('location')['price'].sum().sort_values(ascending=False).head(10)\n# Generate Top 10 categories by sales amount\ntop_categories = data.groupby('master_category_full_name')['price'].sum().sort_values(ascending=False).head(10)\ntop_category_names = top_categories.index.tolist()\n# Generate forecasts for each top product\nreport_date = datetime.now().strftime('%Y-%m-%d')\nwith open('sales_report.md', 'w', encoding='utf-8') as f:\n    f.write(\"# Result of Analyzing and Forecasting of Your Products\\n\\n\")",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "top_locations",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "top_locations = data.groupby('location')['price'].sum().sort_values(ascending=False).head(10)\n# Generate Top 10 categories by sales amount\ntop_categories = data.groupby('master_category_full_name')['price'].sum().sort_values(ascending=False).head(10)\ntop_category_names = top_categories.index.tolist()\n# Generate forecasts for each top product\nreport_date = datetime.now().strftime('%Y-%m-%d')\nwith open('sales_report.md', 'w', encoding='utf-8') as f:\n    f.write(\"# Result of Analyzing and Forecasting of Your Products\\n\\n\")\n    # Top 10 Product Forecasting Section\n    f.write(\"## Top 10 Sales Amount Product Forecasting\\n\\n\")",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "top_categories",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "top_categories = data.groupby('master_category_full_name')['price'].sum().sort_values(ascending=False).head(10)\ntop_category_names = top_categories.index.tolist()\n# Generate forecasts for each top product\nreport_date = datetime.now().strftime('%Y-%m-%d')\nwith open('sales_report.md', 'w', encoding='utf-8') as f:\n    f.write(\"# Result of Analyzing and Forecasting of Your Products\\n\\n\")\n    # Top 10 Product Forecasting Section\n    f.write(\"## Top 10 Sales Amount Product Forecasting\\n\\n\")\n    for idx, product_desc in enumerate(top_product_descriptions, 1):\n        product_data = data[data['item_description'] == product_desc]",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "top_category_names",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "top_category_names = top_categories.index.tolist()\n# Generate forecasts for each top product\nreport_date = datetime.now().strftime('%Y-%m-%d')\nwith open('sales_report.md', 'w', encoding='utf-8') as f:\n    f.write(\"# Result of Analyzing and Forecasting of Your Products\\n\\n\")\n    # Top 10 Product Forecasting Section\n    f.write(\"## Top 10 Sales Amount Product Forecasting\\n\\n\")\n    for idx, product_desc in enumerate(top_product_descriptions, 1):\n        product_data = data[data['item_description'] == product_desc]\n        product_data = product_data.sort_values('purchase_date')",
        "detail": "report_generation",
        "documentation": {}
    },
    {
        "label": "report_date",
        "kind": 5,
        "importPath": "report_generation",
        "description": "report_generation",
        "peekOfCode": "report_date = datetime.now().strftime('%Y-%m-%d')\nwith open('sales_report.md', 'w', encoding='utf-8') as f:\n    f.write(\"# Result of Analyzing and Forecasting of Your Products\\n\\n\")\n    # Top 10 Product Forecasting Section\n    f.write(\"## Top 10 Sales Amount Product Forecasting\\n\\n\")\n    for idx, product_desc in enumerate(top_product_descriptions, 1):\n        product_data = data[data['item_description'] == product_desc]\n        product_data = product_data.sort_values('purchase_date')\n        sales_values = product_data[['price']].values\n        # Prepare data for forecasting",
        "detail": "report_generation",
        "documentation": {}
    }
]